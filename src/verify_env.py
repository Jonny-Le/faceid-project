import os, time, platform
import torch
import torch.nn as nn
from torch import amp

# ============================================================
# verify_env.py â€“ Environment and GPU benchmark check
# Kiá»ƒm tra toÃ n bá»™ mÃ´i trÆ°á»ng AI vÃ  hiá»‡u nÄƒng GPU
# ============================================================

def print_header(title_en, title_vi):
    print("\n" + "="*100)
    print(f"{title_en}\n{title_vi}")
    print("="*100)

def main():
    # ------------------------------------------------------------
    # Clear screen before running (for readability)
    # XÃ³a toÃ n bá»™ cá»­a sá»• trÆ°á»›c khi in káº¿t quáº£
    # ------------------------------------------------------------
    os.system('clear')
    print("ğŸ§  PyTorch Environment Verification Started")
    print("ğŸ” Kiá»ƒm tra mÃ´i trÆ°á»ng PyTorch vÃ  GPU báº¯t Ä‘áº§u...\n")

    # ------------------------------------------------------------
    # Step 1 â€“ Configure TF32 (TensorFloat32) precision
    # Báº­t TF32 Ä‘á»ƒ tÄƒng tá»‘c FP32 trÃªn Tensor Cores (náº¿u GPU há»— trá»£)
    # ------------------------------------------------------------
    try:
        torch.backends.cudnn.conv.fp32_precision = "tf32"
        torch.backends.cuda.matmul.allow_tf32 = True
        print("âœ… TF32 mode enabled for both cuBLAS (GEMM) and cuDNN (Conv).")
        print("   ÄÃ£ báº­t cháº¿ Ä‘á»™ TF32 cho cáº£ GEMM vÃ  Convolution.\n")
    except Exception as e:
        print("âš ï¸  Could not fully enable TF32:", e, "\n")

    # ------------------------------------------------------------
    # Step 2 â€“ Print environment summary
    # In thÃ´ng tin tÃ³m táº¯t mÃ´i trÆ°á»ng
    # ------------------------------------------------------------
    print_header("ENVIRONMENT INFO", "THÃ”NG TIN MÃ”I TRÆ¯á»œNG")
    print("Python Version     :", platform.python_version())
    print("PyTorch Version    :", torch.__version__)
    print("CUDA Toolkit       :", torch.version.cuda)
    print("CUDA Available     :", torch.cuda.is_available())

    if not torch.cuda.is_available():
        print("âŒ No CUDA device found! / KhÃ´ng phÃ¡t hiá»‡n GPU CUDA.")
        return

    print("GPU Device         :", torch.cuda.get_device_name(0))
    print("Compute Capability :", torch.cuda.get_device_capability(0))
    print("CUDA Arch List     :", os.environ.get("TORCH_CUDA_ARCH_LIST", "(not set)"))
    print("cuDNN Version      :", torch.backends.cudnn.version())
    print("NCCL Available     :", torch.distributed.is_nccl_available())

    # ------------------------------------------------------------
    # Step 3 â€“ TF32 status check
    # Kiá»ƒm tra tráº¡ng thÃ¡i TF32 hiá»‡n táº¡i
    # ------------------------------------------------------------
    print_header("TF32 SETTINGS", "CÃ€I Äáº¶T TF32")
    print("cuDNN Conv FP32 Precision :", torch.backends.cudnn.conv.fp32_precision)
    print("cuBLAS Matmul allow_tf32  :", torch.backends.cuda.matmul.allow_tf32)

    # ------------------------------------------------------------
    # Step 4 â€“ Check channels_last memory format
    # Kiá»ƒm tra Ä‘á»‹nh dáº¡ng bá»™ nhá»› channels_last
    # ------------------------------------------------------------
    print_header("CHANNELS_LAST FORMAT CHECK", "KIá»‚M TRA Äá»ŠNH Dáº NG Bá»˜ NHá»š channels_last")
    x = torch.randn(2, 3, 224, 224)
    print("Default stride (NCHW):", x.stride())
    x_cl = x.to(memory_format=torch.channels_last)
    print("Channels_last stride:", x_cl.stride())

    # ------------------------------------------------------------
    # Step 5 â€“ GEMM benchmark (matrix multiply)
    # Kiá»ƒm tra tá»‘c Ä‘á»™ nhÃ¢n ma tráº­n (Ä‘Æ¡n vá»‹: TFLOP/s)
    # ------------------------------------------------------------
    print_header("GEMM BENCHMARK", "KIá»‚M TRA HIá»†U NÄ‚NG GEMM (NHÃ‚N MA TRáº¬N)")
    device = "cuda"
    sizes = [(4096,4096,4096)]
    dtypes = [torch.float16, torch.bfloat16, torch.float32]

    for (M,N,K) in sizes:
        for dt in dtypes:
            try:
                a = torch.randn(M,K, device=device, dtype=dt)
                b = torch.randn(K,N, device=device, dtype=dt)
                for _ in range(3):  # warmup
                    with amp.autocast('cuda', dtype=dt):
                        (a @ b).relu_()
                    torch.cuda.synchronize()
                torch.cuda.synchronize(); t0 = time.time()
                with amp.autocast('cuda', dtype=dt):
                    c = a @ b
                torch.cuda.synchronize(); dt_ms = (time.time() - t0) * 1000
                tflops = (2*M*N*K) / ((dt_ms/1000)*1e12)
                print(f"{str(dt).split('.')[-1]:>9}  {M}x{K} @ {K}x{N}  time={dt_ms:7.2f} ms  ~{tflops:6.2f} TFLOP/s")
            except Exception as e:
                print(f"âŒ {str(dt).split('.')[-1]} ERROR:", e)

    # ------------------------------------------------------------
    # Step 6 â€“ Convolution benchmark (like CNN layer)
    # Kiá»ƒm tra tá»‘c Ä‘á»™ convolution 3x3 (nhÆ° trong máº¡ng CNN)
    # ------------------------------------------------------------
    print_header("CONV2D BENCHMARK", "KIá»‚M TRA HIá»†U NÄ‚NG CONVOLUTION 2D")
    for dt in [torch.float16, torch.bfloat16, torch.float32]:
        try:
            conv = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1).to(device)
            x = torch.randn(32, 64, 112, 112, device=device).to(memory_format=torch.channels_last)
            for _ in range(5):  # warmup
                with amp.autocast('cuda', dtype=dt):
                    y = conv(x)
                torch.cuda.synchronize()
            torch.cuda.synchronize(); t0 = time.time()
            with amp.autocast('cuda', dtype=dt):
                y = conv(x)
            torch.cuda.synchronize(); dt_ms = (time.time() - t0) * 1000
            print(f"{str(dt).split('.')[-1]:>9}  Conv(64â†’128, 3x3)  time={dt_ms:6.2f} ms")
        except Exception as e:
            print(f"âŒ {str(dt).split('.')[-1]} ERROR:", e)

    # ------------------------------------------------------------
    # Step 7 â€“ torch.compile functional test
    # Kiá»ƒm tra tÃ­nh nÄƒng biÃªn dá»‹ch Ä‘á»™ng cá»§a PyTorch
    # ------------------------------------------------------------
    print_header("torch.compile TEST", "KIá»‚M TRA torch.compile (BiÃªn dá»‹ch Ä‘á»™ng)")
    try:
        m = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, 1, 1)
        ).cuda()
        m_compiled = torch.compile(m)
        x = torch.randn(8,3,224,224, device="cuda").to(memory_format=torch.channels_last)
        with amp.autocast('cuda', dtype=torch.float16):
            y = m_compiled(x)
        torch.cuda.synchronize()
        print("âœ… torch.compile works correctly (OK)")
        print("   torch.compile hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng.")
    except Exception as e:
        print("âš ï¸ torch.compile skipped or failed:", e)

    # ------------------------------------------------------------
    # Step 8 â€“ Summary
    # ------------------------------------------------------------
    print_header("SUMMARY", "Tá»”NG Káº¾T")
    print("âœ… If all sections show OK or reasonable speeds -> Environment is READY.")
    print("   Náº¿u cÃ¡c pháº§n kiá»ƒm tra Ä‘á»u OK hoáº·c tá»‘c Ä‘á»™ há»£p lÃ½ â†’ MÃ´i trÆ°á»ng Ä‘Ã£ sáºµn sÃ ng cho AI training.\n")
    print("ğŸ Verification completed successfully.")
    print("   QuÃ¡ trÃ¬nh kiá»ƒm tra Ä‘Ã£ hoÃ n táº¥t thÃ nh cÃ´ng.")

if __name__ == "__main__":
    main()
